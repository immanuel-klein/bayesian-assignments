---
title: "Assignment 5"
format: pdf
author:
  - name: "Kerem Karag√∂z"
  - name: "Immanuel Klein"
execute: 
  cache: true
  warning: false
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: sentence
---

GitHub: <https://github.com/immanuel-klein/bayesian-assignments.git>

```{r packages}
# load packages here
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tinytex)
library(rethinking)
library(rstan)
library(reshape2)
library(ggridges)
```

\newpage

# Task Set 1

Load the data set `RiskyChoice.csv` to solve the Task Set 1.
Use the `read_csv2()` function instead of `read_csv()`.

```{r data}
# load data here
# Used read.csv2 instead of read_csv2 
# to correctly get the decimal seperators for NegativeAffect
# Then use as numerics to convert to num from character
risky <- read.csv2("RiskyChoice.csv")
risky$NegativeAffect <- as.numeric(risky$NegativeAffect)
head(risky)
```

## Task 1.1

Create a reduced data table with only one row per subject that shows the number of solved choices problems (`nChoice`) and the number of correct choices (`nCorrect`) for each subject along with the other variables.
Remove the subjects with missing values.
Print the data of the first 10 subjects.

```{r}
# write code here
risky.reduced <- risky %>%
  filter(!is.na(CorrectChoice)) %>%
  group_by(Subject) %>%
  summarise(
    Gender = first(Gender),
    NegativeAffect = first(NegativeAffect),
    Numeracy = first(Numeracy),
    nChoice = n(),
    nCorrect = sum(CorrectChoice)
  ) %>% na.omit()

head(risky.reduced, 10)
```

**Remark:** We understood the task the following way: If a subject has at least one NA value for CorrectChoice, do not take any values of this subject into the new table.
However, this left us with a tibble with the dimensions 0x3, i.
e.
there is no subject that has no NA value for CorrectChoice.
Thus, we decided to just filter the rows with NA for CorrectChoice instead of the whole subject.
Also, because risky.reduced should only have one row per subject, we could only include Gender, NegativeAffect, and Numeracy.
The others have differing values within the subjects.
We then omit NAs.

## Task 1.2

Run a Bayesian regression model that predicts `nCorrect` from `Numeracy` using fixed intercepts and fixed slopes.
Standardize the predictor before running the model and compute the WAIC of the model.

```{r m1}
#| echo: true
#| eval: true
#| output: false

# write data list and model here
risky.reduced.regr <- risky.reduced %>%
  mutate(
    numeracy_std = (Numeracy - mean(Numeracy)) / sd(Numeracy),
    # Convert Subject to a numeric index
    subject_idx = as.numeric(as.factor(Subject))
)

data.list <- list(
  nCorrect = risky.reduced.regr$nCorrect,
  nChoice = risky.reduced.regr$nChoice,
  numeracy_std = risky.reduced.regr$numeracy_std
)

m1 <- ulam(
  alist(
    nCorrect ~ dbinom(nChoice, p),
    logit(p) <- a + b*numeracy_std,
    # Choosing intercept and slope priors:
    # When numeracy is 0, it's reasonable that correct choices come from guessing.
    # Thus, ca 50% of all choices might be correct (0.5 of ~ 100 choices).
    # Looking at the values where numeracy is 10, a 0.3 slope might make sense.
    a ~ dnorm(50, 10),
    b ~ dnorm(0.3, 0.1)
  ), data = data.list, chains = 4, cores = 4, log_lik = TRUE
)
```

```{r}
#write code here
WAIC(m1)
```

## Task 1.3

Run a Bayesian regression model that predicts `nCorrect` from `Numeracy` using random intercepts and fixed slopes.
Standardize the predictor before running the model and compute the WAIC of the model.

```{r m2}
#| echo: true
#| eval: true
#| output: false

# write data list and model here

data.list2 <- list(
  nCorrect = risky.reduced.regr$nCorrect,
  nChoice = risky.reduced.regr$nChoice,
  numeracy_std = risky.reduced.regr$numeracy_std,
  subject = risky.reduced.regr$subject_idx
)

m2 <- ulam(
  alist(
    nCorrect ~ dbinom(nChoice, p),
    logit(p) <- a[subject] + beta*numeracy_std,
    a[subject] ~ dnorm(a_bar, tau_a),  
    a_bar ~ dnorm(50, 10),
    tau_a ~ dexp(0.5),
    beta ~ dnorm(0.3, 0.1)
  ), data = data.list2, chains = 4, cores = 4, log_lik = TRUE
)
```

```{r}
# write code here
WAIC(m2)
```

\newpage

# Task Set 2

## Task 2.1

Create a data table that entails 10,000 posterior samples (rows) for each subject-specific (columns) intercept.
Convert the sampled values into probabilities and print the first 10 samples of the first 10 subjects.

```{r}
# write code here
samples <- extract.samples(m2, n = 10000) 

prob.samples <- inv_logit(samples$a)

prob.samples.df <- as.data.frame(prob.samples)
colnames(prob.samples.df) <- 
  paste0("Subject ", 1:ncol(prob.samples.df))

head(prob.samples.df[, 1:10], 10)

```

## Task 2.2

Use the posterior samples to plot the posterior distribution of all subject-specific intercepts to show the variability in the performance among subjects.
Use the converted values (probabilities).

```{r}
# write code here
# As there are 119 different subjects it is quite hard to visualize the intercepts

# Convert the wide format to long format for ggplot2
prob.samples.long <- melt(prob.samples.df)

# Plot the posterior distributions
ggplot(prob.samples.long, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  labs(title = "Posterior Distributions of Subject-Specific Intercepts",
       x = "Probability", y = "Density") +
  theme(legend.position = "none")

# An alternative version where we only use the first 10 subjects
prob.samples.subset <- prob.samples.df[, 1:10]
prob.samples.melt.subset <- melt(prob.samples.subset)

# Create a faceted plot for the first 10 subjects
ggplot(prob.samples.melt.subset, aes(x = value)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ variable, scales = "free_y") +
  labs(title = "Posterior Distributions of Subject-Specific Intercepts",
       x = "Probability", y = "Density")


```

## Task 2.3

Consider the following posterior summaries and traceplots.
Which model was estimated and what might be the cause of the convergence problems?

```{r}
# precis(m3)
# traceplot_ulam(m3, pars = c("mu_a", "tau_a", "mu_b", "tau_b"))
```

The estimated model is most likely a hierarchical Bayesian regression model with random intercepts and slopes.
It includes a Hierarchical structure with hyperparameters mu_a, tau_a, mu_b, and tau_b to model the population-level mean and standard deviation of the intercepts and slopes.

From the precis we can see that mu_a shows a good convergence with high effective sample size and rhat close to 1, meaning that chains are well mixed.
When we go from mu_a to tau_a, mu_b and tau_b in order the effective sample size becomes smaller, rhat value is getting further away from 1 (except tau_a) and showing poorer convergences.
So tau_b shows significant issues during chain mixing and convergence.
Even though tau_a looks quite good from the parameters of precis its traceplot also collapses immediately.

The effective sample size (n_eff) for tau_b is very low (\~44), suggesting that the sampler struggles to produce independent samples.
The rhat value for tau_b is significantly above 1 (\~1.106), indicating that the chains have not converged to the same distribution.
tau_b might be poorly identified, leading to difficulties in sampling.
The data may not provide enough information to reliably estimate this parameter, resulting in convergence problems.

The hierarchical nature of the model, with both random intercepts and slopes, increases the complexity of the parameter space.
This complexity can make it challenging for the sampler to converge, especially if the data do not strongly inform all parameters.

The priors specified for the parameters (especially tau_a and tau_b) might be too restrictive or not well-aligned with the data.
This misalignment can hinder the sampler's ability to explore the parameter space effectively.
